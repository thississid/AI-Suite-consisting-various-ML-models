{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVM Progress:   0%|          | 0/1 [00:00<?, ?it/s]/opt/miniconda3/envs/aienv/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "SVM Progress: 100%|██████████| 1/1 [01:42<00:00, 102.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model, PCA, and scaler saved in 102.09 seconds.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/aienv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3230 - loss: 1.8275"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Progress:   0%|          | 0/10 [00:12<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - loss: 1.5912 - accuracy: 0.4192 - val_loss: 1.2932 - val_accuracy: 0.5402\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.3233 - loss: 1.8269 - val_accuracy: 0.5402 - val_loss: 1.2932\n",
      "Epoch 2/10\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5554 - loss: 1.2511"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Progress:   0%|          | 0/10 [00:23<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - loss: 1.2150 - accuracy: 0.5690 - val_loss: 1.1330 - val_accuracy: 0.5940\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.5555 - loss: 1.2510 - val_accuracy: 0.5940 - val_loss: 1.1330\n",
      "Epoch 3/10\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6153 - loss: 1.0973"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Progress:   0%|          | 0/10 [00:34<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - loss: 1.0705 - accuracy: 0.6245 - val_loss: 1.0259 - val_accuracy: 0.6411\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.6153 - loss: 1.0973 - val_accuracy: 0.6411 - val_loss: 1.0259\n",
      "Epoch 4/10\n",
      "\u001b[1m778/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6550 - loss: 0.9860"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Progress:   0%|          | 0/10 [00:45<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - loss: 0.9788 - accuracy: 0.6565 - val_loss: 1.0426 - val_accuracy: 0.6355\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.6550 - loss: 0.9859 - val_accuracy: 0.6355 - val_loss: 1.0426\n",
      "Epoch 5/10\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6791 - loss: 0.9187"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Progress:   0%|          | 0/10 [00:56<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - loss: 0.9071 - accuracy: 0.6829 - val_loss: 0.9826 - val_accuracy: 0.6593\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.6791 - loss: 0.9186 - val_accuracy: 0.6593 - val_loss: 0.9826\n",
      "Epoch 6/10\n",
      "\u001b[1m778/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6954 - loss: 0.8622"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Progress:   0%|          | 0/10 [01:07<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - loss: 0.8567 - accuracy: 0.7001 - val_loss: 0.9222 - val_accuracy: 0.6767\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.6955 - loss: 0.8621 - val_accuracy: 0.6767 - val_loss: 0.9222\n",
      "Epoch 7/10\n",
      "\u001b[1m779/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7179 - loss: 0.8114"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Progress:   0%|          | 0/10 [01:18<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - loss: 0.8077 - accuracy: 0.7189 - val_loss: 0.9081 - val_accuracy: 0.6846\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.7179 - loss: 0.8114 - val_accuracy: 0.6846 - val_loss: 0.9081\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7316 - loss: 0.7685"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Progress:   0%|          | 0/10 [01:28<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - loss: 0.7633 - accuracy: 0.7351 - val_loss: 0.8414 - val_accuracy: 0.7107\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.7317 - loss: 0.7684 - val_accuracy: 0.7107 - val_loss: 0.8414\n",
      "Epoch 9/10\n",
      "\u001b[1m779/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7457 - loss: 0.7273"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Progress:   0%|          | 0/10 [01:39<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - loss: 0.7277 - accuracy: 0.7463 - val_loss: 0.8751 - val_accuracy: 0.7032\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.7457 - loss: 0.7273 - val_accuracy: 0.7032 - val_loss: 0.8751\n",
      "Epoch 10/10\n",
      "\u001b[1m780/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7550 - loss: 0.6914"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Progress:   0%|          | 0/10 [01:50<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - loss: 0.7003 - accuracy: 0.7533 - val_loss: 0.8553 - val_accuracy: 0.7077\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.7550 - loss: 0.6915 - val_accuracy: 0.7077 - val_loss: 0.8553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Progress:   0%|          | 0/10 [01:50<?, ?it/s]\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model saved in 110.30 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.datasets import cifar10\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm  # Status bar\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 1. Traditional ML Approach: SVM\n",
    "\n",
    "# Flatten images into 1D vectors for SVM\n",
    "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_flattened)\n",
    "X_test_scaled = scaler.transform(X_test_flattened)\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Train SVM classifier with progress bar\n",
    "svm_model = SVC(kernel='rbf', random_state=42, max_iter=1000)\n",
    "\n",
    "print(\"Training SVM model...\")\n",
    "start_time = time.time()\n",
    "for i in tqdm(range(1), desc=\"SVM Progress\"):\n",
    "    svm_model.fit(X_train_pca, y_train.ravel())\n",
    "training_time_svm = time.time() - start_time\n",
    "\n",
    "# Save SVM model, PCA, and scaler\n",
    "with open(\"svm_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(svm_model, f)\n",
    "\n",
    "with open(\"pca_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pca, f)\n",
    "\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"SVM model, PCA, and scaler saved in {training_time_svm:.2f} seconds.\\n\")\n",
    "\n",
    "# 2. Deep Learning Approach: CNN\n",
    "\n",
    "# Normalize the data for CNN\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# One-hot encode the labels for CNN\n",
    "y_train_cnn = to_categorical(y_train, 10)\n",
    "y_test_cnn = to_categorical(y_test, 10)\n",
    "\n",
    "# Build a simple CNN model\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the CNN model\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN model with progress bar\n",
    "print(\"Training CNN model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Use tqdm for progress bar and Keras callback\n",
    "class TQDMProgressCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        tqdm.write(f\"Epoch {epoch+1} - loss: {logs['loss']:.4f} - accuracy: {logs['accuracy']:.4f} - val_loss: {logs['val_loss']:.4f} - val_accuracy: {logs['val_accuracy']:.4f}\")\n",
    "\n",
    "with tqdm(total=10, desc=\"CNN Progress\") as pbar:\n",
    "    history = cnn_model.fit(\n",
    "        X_train, y_train_cnn,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_test, y_test_cnn),\n",
    "        callbacks=[TQDMProgressCallback()]\n",
    "    )\n",
    "training_time_cnn = time.time() - start_time\n",
    "\n",
    "# Save the CNN model\n",
    "cnn_model.save('cnn_model.h5')\n",
    "\n",
    "print(f\"CNN model saved in {training_time_cnn:.2f} seconds.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model -> Accuracy: 0.8517, Training Time: 0.01 seconds\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/aienv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.6536 - loss: 0.5845 - val_accuracy: 0.7965 - val_loss: 0.4447\n",
      "Epoch 2/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.8667 - loss: 0.3229 - val_accuracy: 0.8469 - val_loss: 0.3691\n",
      "Epoch 3/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9093 - loss: 0.2366 - val_accuracy: 0.8617 - val_loss: 0.3489\n",
      "Epoch 4/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9348 - loss: 0.1745 - val_accuracy: 0.8507 - val_loss: 0.3718\n",
      "Epoch 5/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9573 - loss: 0.1242 - val_accuracy: 0.8596 - val_loss: 0.4626\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8641 - loss: 0.4504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Model -> Accuracy: 0.8596, Training Time: 44.02 seconds\n",
      "\n",
      "Comparison of Naive Bayes and RNN:\n",
      "Naive Bayes -> Accuracy: 0.8517, Training Time: 0.01 seconds\n",
      "RNN -> Accuracy: 0.8596, Training Time: 44.02 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Loading and Splitting Dataset\n",
    "# Load dataset\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "# Map sentiment to binary (positive: 1, negative: 0)\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the split datasets for future use\n",
    "train_df = pd.DataFrame({'review': X_train, 'sentiment': y_train})\n",
    "test_df = pd.DataFrame({'review': X_test, 'sentiment': y_test})\n",
    "train_df.to_csv('train_reviews.csv', index=False)\n",
    "test_df.to_csv('test_reviews.csv', index=False)\n",
    "\n",
    "# 2. Traditional ML Approach: Naive Bayes Classifier\n",
    "# Preprocess text using TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Train and time the model\n",
    "start_time = time.time()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "nb_training_time = time.time() - start_time\n",
    "\n",
    "# Test the model\n",
    "nb_predictions = nb_model.predict(X_test_tfidf)\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "\n",
    "# Save the Naive Bayes model and TF-IDF Vectorizer\n",
    "with open(\"nb_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(nb_model, f)\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "print(f\"Naive Bayes Model -> Accuracy: {nb_accuracy:.4f}, Training Time: {nb_training_time:.2f} seconds\")\n",
    "\n",
    "# 3. Deep Learning Approach: Recurrent Neural Network (RNN)\n",
    "# Preprocessing for RNN\n",
    "max_words = 10000\n",
    "max_len = 200\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# Build RNN model\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(input_dim=max_words, output_dim=32, input_length=max_len))\n",
    "rnn_model.add(SimpleRNN(32))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train and time the model\n",
    "start_time = time.time()\n",
    "history = rnn_model.fit(X_train_pad, y_train, epochs=5, batch_size=64, validation_data=(X_test_pad, y_test))\n",
    "rnn_training_time = time.time() - start_time\n",
    "\n",
    "# Evaluate the RNN model\n",
    "rnn_loss, rnn_accuracy = rnn_model.evaluate(X_test_pad, y_test)\n",
    "\n",
    "# Save the RNN model and Tokenizer\n",
    "rnn_model.save('rnn_model.h5')\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(f\"RNN Model -> Accuracy: {rnn_accuracy:.4f}, Training Time: {rnn_training_time:.2f} seconds\")\n",
    "\n",
    "# 4. Summary of Results\n",
    "print(\"\\nComparison of Naive Bayes and RNN:\")\n",
    "print(f\"Naive Bayes -> Accuracy: {nb_accuracy:.4f}, Training Time: {nb_training_time:.2f} seconds\")\n",
    "print(f\"RNN -> Accuracy: {rnn_accuracy:.4f}, Training Time: {rnn_training_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
